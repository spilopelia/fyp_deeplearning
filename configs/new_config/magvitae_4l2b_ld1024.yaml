model:
  learning_rate: 1.0e-4
  weight_decay: 1.0e-4
  optimizer: 'AdamW' # ['Adam', 'AdamW']
  lr_scheduler: 'Constant' # ['Constant', 'Cosine']
  #lr_warmup: 1000 # needed if use cosine lr_scheduler
  #lr_cosine_period: 24000 # match dataset_size/batch_size*max_epochs
  model: 'MagVitAE3D' # ['default', 'UNet', 'VAE']
  num_layers: 4 # layer number of UNet
  blocks_per_layer: 2 # number of convolutional blocks in each layer
  init_dim: 3 # 3 for without density field in tensor
  latent_dim: 1024
  reversed: False # if True, predict ZA from FastPM
  eul_loss: False # if True, use Eulerian loss
  eul_loss_scale: 1.0 # scale factor for Eulerian loss
  lag_loss_scale: 1.0 # scale factor for Lagrangian loss


data:
  dataset_type: 'huggingface' # ['raw', 'huggingface']
  dataset_path: '/home/user/ckwan1/ml/huggingface_dataset'
  batch_size: 128
  num_workers: 16
  augment: True # if True, use data augmentation
  density: False # if True, use density field in tensor

trainer:
  max_epochs: 500
  gpus: 4
  num_nodes: 1

wandb:
  project: "fyp"
  entity: "brianwan221-the-chinese-university-of-hong-kong"
  name: "magvitae_4l2b_ld1024"